{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cells will fill entire width of the browser\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "#Tells Jupyter to reload custom classes from scratch everytime an import cell is run, if you edit a custom class\n",
    "#between imports Jupyter would otherwise need to be restarted completely. Buyer beware: old class objects in the \n",
    "#current namespace will cause errors at execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#switches matplotlib to show plots in the browser rather than opening a new window\n",
    "%matplotlib inline\n",
    "\n",
    "#always forget to do this for better looking plots\n",
    "import seaborn\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from statsmodels.tsa import stattools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(False) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian classifier\n",
    "def mat_C(x, V=False):\n",
    "    #x shape = px1\n",
    "    if type(V) != np.ndarray:\n",
    "        V = np.eye(x.shape[0])\n",
    "    C = np.dot(x, x.T) + V\n",
    "    return(C)\n",
    "\n",
    "def mat_D(x, y, A, V=False):\n",
    "    #x shape px1, y shape nx1, A nxp\n",
    "    if type(V) != np.ndarray:\n",
    "        V = np.eye(x.shape[0])\n",
    "    D = np.dot(y, x.T) + A.dot(V)\n",
    "    return(D)\n",
    "\n",
    "def likelihood_point(x, y, A, V=False):\n",
    "    C = mat_C(x, V)\n",
    "    D = mat_D(x, y, A, V)\n",
    "    C_inv = np.linalg.inv(C)\n",
    "    const = x.shape[0]*np.log(np.linalg.det(C_inv))\n",
    "    var = np.trace(y.dot(y.T) - 2*A.dot(x).dot(y.T) + x.T.dot(A.T).dot(A).dot(x))\n",
    "    var2 = np.trace(y.dot(y.T) + A.dot(A.T) - D.dot(C_inv).dot(D.T))   #mixed term doesn't look right\n",
    "    out = var - var2 + const\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize A, PD\n",
    "A = np.array([[0.9, 0.0],\n",
    "              [0.0, -0.4]])\n",
    "\n",
    "V = np.array([[1.0, -0.001],\n",
    "              [-0.001, 0.02]])\n",
    "\n",
    "V_inv = np.linalg.inv(V)\n",
    "\n",
    "lags = [10]\n",
    "MC = 100#500\n",
    "num_samps = 500\n",
    "\n",
    "num_samps_list = np.arange(0,20,1)#[0,5,10,20,50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000102, 0.01999898]), array([[ 0.99999948,  0.00102041],\n",
       "        [-0.00102041,  0.99999948]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag round:  10\n",
      "Matrix initialization round:  0\n",
      "Matrix initialization round:  10\n",
      "Matrix initialization round:  20\n",
      "Matrix initialization round:  30\n",
      "Matrix initialization round:  40\n",
      "Matrix initialization round:  50\n"
     ]
    }
   ],
   "source": [
    "mc_output_by_lag = {}\n",
    "\n",
    "for M in lags:\n",
    "    print(\"Lag round: \", M)\n",
    "    mc_output_by_lag[M] = np.zeros((MC,3))\n",
    "    for j in range(MC):\n",
    "        if j % 10 == 0:\n",
    "            print(\"Matrix initialization round: \", j)\n",
    "        #B = A + randomly sample an element of A to perturb by N(0,1)\n",
    "        #e = np.unravel_index(np.random.choice([i for i in range(A.shape[0]*A.shape[1])]), dims=A.shape)\n",
    "        B = copy.copy(A) + np.vstack((np.random.multivariate_normal(np.array([0,0]),V), np.random.multivariate_normal(np.array([0,0]),V)))\n",
    "        #B[e] += np.random.normal(0,1)\n",
    "        mc_output_by_lag[M][j,0] = np.linalg.norm(B-A,'fro')\n",
    "        \n",
    "        #generate a bunch of samples\n",
    "        X_s = np.random.normal(size=(2,num_samps))##, size=(2,1000))\n",
    "        Y_s = A.dot(X_s) + np.asarray([ np.random.normal(0, np.eye(2))[:,0] for i in range(num_samps) ]).T\n",
    "        Y_s_b = B.dot(X_s) + np.asarray([ np.random.normal(0, np.eye(2))[:,0] for i in range(num_samps) ]).T\n",
    "        \n",
    "        f_bin = []\n",
    "        nf_bin = []\n",
    "        \n",
    "        for i in range(X_s.shape[1] - M):\n",
    "            #for each sample\n",
    "            nf_sum = [0.0]\n",
    "            f_sum = [0.0]\n",
    "            \n",
    "\n",
    "            for k in range(M):\n",
    "                #collect likelihood across lag\n",
    "                nf = likelihood_point(np.expand_dims(X_s[:,i+k], axis=1), np.expand_dims(Y_s[:,i+k], axis=1), A)\n",
    "                f = likelihood_point(np.expand_dims(X_s[:,i+k], axis=1), np.expand_dims(Y_s_b[:,i+k], axis=1), A)\n",
    "\n",
    "                nf_sum.append(nf_sum[-1] + nf)\n",
    "                f_sum.append(f_sum[-1] + f)\n",
    "\n",
    "\n",
    "            if f_sum[-1] > 0:\n",
    "                f_bin.append(1)\n",
    "            else:\n",
    "                f_bin.append(0)\n",
    "\n",
    "            if nf_sum[-1] > 0:\n",
    "                nf_bin.append(1)\n",
    "            else:\n",
    "                nf_bin.append(0)\n",
    "                    \n",
    "        pred = np.concatenate((f_bin, nf_bin))\n",
    "        true = np.concatenate((np.ones(np.asarray(f_bin).shape), np.zeros(np.asarray(nf_bin).shape)))\n",
    "\n",
    "        p = sklearn.metrics.precision_score(true, pred)\n",
    "        r = sklearn.metrics.recall_score(true, pred)\n",
    "                \n",
    "        mc_output_by_lag[M][j,1] = p\n",
    "        mc_output_by_lag[M][j,2] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_output_by_lag_skew = {}\n",
    "\n",
    "for M in lags:\n",
    "    print(\"Lag round: \", M)\n",
    "    mc_output_by_lag_skew[M] = np.zeros((MC,3))\n",
    "    for j in range(MC):\n",
    "        if j % 10 == 0:\n",
    "            print(\"Matrix initialization round: \", j)\n",
    "        #B = A + randomly sample an element of A to perturb by N(0,1)\n",
    "        #e = np.unravel_index(np.random.choice([i for i in range(A.shape[0]*A.shape[1])]), dims=A.shape)\n",
    "        B = copy.copy(A) + np.vstack((np.random.multivariate_normal(np.array([0,0]),V), np.random.multivariate_normal(np.array([0,0]),V)))\n",
    "        #B[e] += np.random.normal(0,1)\n",
    "        mc_output_by_lag_skew[M][j,0] = np.linalg.norm(B-A,'fro')\n",
    "        \n",
    "        #generate a bunch of samples\n",
    "        X_s = np.random.normal(size=(2,num_samps))##, size=(2,1000))\n",
    "        Y_s = A.dot(X_s) + np.asarray([ np.random.normal(0, np.eye(2))[:,0] for i in range(num_samps) ]).T\n",
    "        Y_s_b = B.dot(X_s) + np.asarray([ np.random.normal(0, np.eye(2))[:,0] for i in range(num_samps) ]).T\n",
    "        \n",
    "        f_bin = []\n",
    "        nf_bin = []\n",
    "        \n",
    "        for i in range(X_s.shape[1] - M):\n",
    "            #for each sample\n",
    "            nf_sum = [0.0]\n",
    "            f_sum = [0.0]\n",
    "            \n",
    "\n",
    "            for k in range(M):\n",
    "                #collect likelihood across lag\n",
    "                nf = likelihood_point(np.expand_dims(X_s[:,i+k], axis=1), np.expand_dims(Y_s[:,i+k], axis=1), A, V_inv)\n",
    "                f = likelihood_point(np.expand_dims(X_s[:,i+k], axis=1), np.expand_dims(Y_s_b[:,i+k], axis=1), A, V_inv)\n",
    "\n",
    "                nf_sum.append(nf_sum[-1] + nf)\n",
    "                f_sum.append(f_sum[-1] + f)\n",
    "\n",
    "\n",
    "            if f_sum[-1] > 0:\n",
    "                f_bin.append(1)\n",
    "            else:\n",
    "                f_bin.append(0)\n",
    "\n",
    "            if nf_sum[-1] > 0:\n",
    "                nf_bin.append(1)\n",
    "            else:\n",
    "                nf_bin.append(0)\n",
    "                    \n",
    "        pred = np.concatenate((f_bin, nf_bin))\n",
    "        true = np.concatenate((np.ones(np.asarray(f_bin).shape), np.zeros(np.asarray(nf_bin).shape)))\n",
    "\n",
    "        p = sklearn.metrics.precision_score(true, pred)\n",
    "        r = sklearn.metrics.recall_score(true, pred)\n",
    "                \n",
    "        mc_output_by_lag_skew[M][j,1] = p\n",
    "        mc_output_by_lag_skew[M][j,2] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_range = np.arange(0.0,3.5,0.1) \n",
    "\n",
    "M = lags[0]\n",
    "clf = None #clear previous model\n",
    "clf = KernelRidge(kernel=\"rbf\", gamma = 1, degree=7, alpha=0.01)\n",
    "f1 = 2*((mc_output_by_lag[M][:,1]*mc_output_by_lag[M][:,2])/(mc_output_by_lag[M][:,1] + mc_output_by_lag[M][:,2]))\n",
    "plt.scatter(mc_output_by_lag[M][:,0], f1, marker=\"o\", color=seaborn.xkcd_rgb[\"pale red\"], s=10)\n",
    "clf.fit(mc_output_by_lag[M][:,0].reshape(-1, 1), f1) \n",
    "\n",
    "r_curve = clf.predict(X_range.reshape(-1, 1))\n",
    "r_curve[r_curve > 1] = 1\n",
    "\n",
    "plt.plot(X_range, r_curve, color=seaborn.xkcd_rgb[\"pale red\"], label=\"Identity covariance prior\" + str(M), lw=2)\n",
    "\n",
    "\n",
    "clf = None #clear previous model\n",
    "clf = KernelRidge(kernel=\"rbf\", gamma = 1, degree=7, alpha=0.01)\n",
    "f1 = 2*((mc_output_by_lag_skew[M][:,1]*mc_output_by_lag_skew[M][:,2])/(mc_output_by_lag_skew[M][:,1] + mc_output_by_lag_skew[M][:,2]))\n",
    "plt.scatter(mc_output_by_lag_skew[M][:,0], f1, marker=\"o\", color=seaborn.xkcd_rgb[\"denim blue\"], s=10)\n",
    "clf.fit(mc_output_by_lag_skew[M][:,0].reshape(-1, 1), f1) \n",
    "\n",
    "r_curve = clf.predict(X_range.reshape(-1, 1))\n",
    "r_curve[r_curve > 1] = 1\n",
    "\n",
    "plt.plot(X_range, r_curve, color=seaborn.xkcd_rgb[\"denim blue\"], label=\"Informed covariance prior\" + str(M), lw=2)\n",
    "    \n",
    "#plt.title(\"F1 Score vs divergence of\\n Monte Carlo realizations of B\", fontsize=14)\n",
    "plt.xlabel(\"$||A - B||_{F}$\", fontsize=14)\n",
    "plt.ylabel(\"F1 Score\", fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc=4)\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlim(0.0,3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "conda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
