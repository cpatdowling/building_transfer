{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cells will fill entire width of the browser\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "#Tells Jupyter to reload custom classes from scratch everytime an import cell is run, if you edit a custom class\n",
    "#between imports Jupyter would otherwise need to be restarted completely. Buyer beware: old class objects in the \n",
    "#current namespace will cause errors at execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#switches matplotlib to show plots in the browser rather than opening a new window\n",
    "%matplotlib inline\n",
    "\n",
    "#always forget to do this for better looking plots\n",
    "import seaborn\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from cvxpy import *\n",
    "from statsmodels.tsa import stattools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class linear_nnet(nn.Module):\n",
    "    #linear model for kernelized inputs\n",
    "    def __init__(self, params):\n",
    "        super(linear_nnet, self).__init__()\n",
    "        self.D_in = params['FEATURE_DIM']\n",
    "        self.D_out = params['OUTPUT_DIM']\n",
    "        self.l1 = nn.Linear(self.D_in, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x) #linear weights for model interpretability\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bayesian classifier\n",
    "def mat_C(x):\n",
    "    #x shape = px1\n",
    "    C = np.dot(x, x.T) + np.eye(x.shape[0])\n",
    "    return(C)\n",
    "\n",
    "def mat_D(x, y, A):\n",
    "    #x shape px1, y shape nx1, A nxp\n",
    "    D = np.dot(y, x.T) + A\n",
    "    return(D)\n",
    "\n",
    "def likelihood_point(x, y, A):\n",
    "    C = mat_C(x)\n",
    "    D = mat_D(x, y, A)\n",
    "    const = x.shape[0]*np.log(np.linalg.det(np.linalg.inv(C)))\n",
    "    var = np.trace(y.dot(y.T) - 2*A.dot(x).dot(y.T) + x.T.dot(A.T).dot(A).dot(x))\n",
    "    var2 = np.trace(y.dot(y.T) + A.dot(A.T) - D.dot(np.linalg.inv(C)).dot(D.T))   #mixed term doesn't look right\n",
    "    out = var - var2 + const\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features to ignore\"\n",
    "ignore = [\"Performance Curve Input Variable 1 Value [] !Each Call\", \"Performance Curve Output Value [] !Each Call\"]\n",
    "\n",
    "#def parse_line()\n",
    "\n",
    "def read_data_dictionary(lines):\n",
    "    data_dict = {}\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        if l == \"End of Data Dictionary\":\n",
    "            break\n",
    "        else:\n",
    "            tokens = l.split(\",\")\n",
    "            if int(tokens[0]) == 2:\n",
    "                data_dict[int(tokens[0])] = tokens[1:]\n",
    "                #parse time dictionary line\n",
    "            elif int(tokens[0]) > 6 and int(tokens[1]) == 1:\n",
    "                if tokens[-1] in ignore:\n",
    "                    pass\n",
    "                elif len(tokens) == 4:\n",
    "                    data_dict[int(tokens[0])] = tokens[-2] + tokens[-1]\n",
    "                    \n",
    "                else:\n",
    "                    #parse value dictionary line\n",
    "                    data_dict[int(tokens[0])] = tokens[-1]\n",
    "            else:\n",
    "                pass\n",
    "    return(data_dict)\n",
    "\n",
    "def parse_time_line(line_str):\n",
    "    #2,8,Day of Simulation[],Month[],Day of Month[],DST Indicator[1=yes 0=no],Hour[],StartMinute[],EndMinute[],DayType\n",
    "    #2, 1, 1, 1, 0, 1, 0.00, 2.00, Holiday\n",
    "    #only want hourly times, multiple minute 2's, assuming got correct\n",
    "    tokens = line_str.strip().split(\",\")\n",
    "    minute=float(tokens[-3])\n",
    "    hour=int(tokens[-4])\n",
    "    day=int(tokens[-6])\n",
    "    month=int(tokens[-7])\n",
    "    return(datetime.datetime(2017, month, day, hour-1, 0, 0)) #2017 so that the 1st is a Sunday + not leapyear\n",
    "    #return(str(month) + \"/\" + str(day) + \" \" + str(hour) + \":00\")\n",
    "     \n",
    "def parse_lines(lines, feature_inds):\n",
    "    data_values = {}\n",
    "    data_values[2] = []\n",
    "    for i in feature_inds:\n",
    "        data_values[i] = []\n",
    "    \n",
    "    start_i = lines.index(\"End of Data Dictionary\\n\")\n",
    "    for line in lines[start_i+1:-2]:\n",
    "        tokens = line.strip().split(\",\")\n",
    "        if int(tokens[0]) == 2:\n",
    "            if float(tokens[-3]) == 0.00 and float(tokens[-2]) == 60.00:\n",
    "                t = parse_time_line(line)\n",
    "                data_values[2].append(t)\n",
    "        elif int(tokens[0]) in feature_inds:\n",
    "            data_values[int(tokens[0])].append(float(tokens[-1]))\n",
    "        else:\n",
    "            pass\n",
    "    return(data_values)\n",
    "    #return hourly list for all feature vars, plus hourly list with times to check sorting\n",
    "    \n",
    "def get_holidays(lines):\n",
    "    holidays = []\n",
    "    weekends = []\n",
    "    all_dates = []\n",
    "    \n",
    "    start_i = lines.index(\"End of Data Dictionary\\n\")\n",
    "    for line in lines[start_i+1:-2]:\n",
    "        tokens = line.strip().split(\",\")\n",
    "        if tokens[-1] == \"Holiday\":\n",
    "            if float(tokens[-3]) == 0.00 and float(tokens[-2]) == 60.00:\n",
    "                t = parse_time_line(line)\n",
    "                holidays.append(t)\n",
    "        elif int(tokens[0]) == 2:\n",
    "            if float(tokens[-3]) == 0.00 and float(tokens[-2]) == 60.00:\n",
    "                t = parse_time_line(line)\n",
    "                all_dates.append(t)\n",
    "                if t.weekday() == 6 or t.weekday() == 0:\n",
    "                    weekends.append(t)\n",
    "    return(all_dates, weekends, holidays)\n",
    "                      \n",
    "def norm_array(arr):\n",
    "    return(preprocessing.minmax_scale(arr, axis=1))\n",
    "\n",
    "def read_total_data_array(filepath, dep_var_names, ind_var_names, zone_temp_inds, first_diff=False, no_weekend_holiday=False):\n",
    "    with open(datapath + filepath) as d:\n",
    "        lines = d.readlines()\n",
    "        lines.pop(0)\n",
    "        data_dict = read_data_dictionary(lines)\n",
    "        data = parse_lines(lines, data_dict)\n",
    "        all_dates, weekends, holidays = get_holidays(lines)\n",
    "        ind_map = {}\n",
    "        for key, value in data_dict.items():\n",
    "            if type(value) == list:\n",
    "                value = value[-1]\n",
    "            ind_map[value] = key\n",
    "\n",
    "    X = np.zeros((len(dep_var_names) + len(zone_temp_inds), 8760))\n",
    "    U = np.zeros((len(ind_var_names), 8760))\n",
    "\n",
    "    for i in range(len(ind_var_names)):\n",
    "        item = ind_var_names[i]\n",
    "        ind = ind_map[item]\n",
    "        if item == 'DayType':\n",
    "            lis = data[ind]\n",
    "            out = []\n",
    "            for val in lis:\n",
    "                out.append(val.weekday())\n",
    "            arr = np.array(out)\n",
    "        else:\n",
    "            arr = np.array(data[ind])\n",
    "        U[i,:] = arr\n",
    "\n",
    "    for j in range(len(dep_var_names)):\n",
    "        item = dep_var_names[j]\n",
    "        ind = ind_map[item]\n",
    "        arr = np.array(data[ind])\n",
    "        X[j,:] = arr\n",
    "\n",
    "    for j in range(len(zone_temp_inds)):\n",
    "        ind = zone_temp_inds[j]\n",
    "        arr = np.array(data[ind])\n",
    "        X[j+len(dep_var_names),:] = arr\n",
    "        \n",
    "    if no_weekend_holiday == True:\n",
    "        weekend_i = []\n",
    "        holiday_i = []\n",
    "        for j in range(len(all_dates)):\n",
    "            if all_dates[j] in weekends:\n",
    "                weekend_i.append(j)\n",
    "            if all_dates[j] in holidays:\n",
    "                holiday_i.append(j)\n",
    "                \n",
    "        del_i = np.array(list(set(holiday_i + weekend_i)))\n",
    "        X = np.delete(X, del_i, axis=1)\n",
    "        U = np.delete(U, del_i, axis=1)\n",
    "    \n",
    "    if first_diff == True:\n",
    "        X = np.diff(X, n=1, axis=1)\n",
    "        U = np.diff(U, n=1, axis=1)\n",
    "\n",
    "    Z = np.vstack((X, U))\n",
    "\n",
    "    return(Z, X, U)\n",
    "\n",
    "def minibatch_X_Y_arrays(X_arr, Y_arr, batchsize):    \n",
    "    #list of training, target pair tuples\n",
    "    remainder = X_arr.shape[1] % batchsize\n",
    "    diff = batchsize - remainder\n",
    "    tail_X = X_arr[:,-diff:] \n",
    "    tail_Y = Y_arr[:,-diff:]\n",
    "    out_X = [ X_arr[:,i*batchsize:(i+1)*batchsize] for i in range(int(float(X_arr.shape[1])/float(batchsize))) ]\n",
    "    out_Y = [ Y_arr[:,i*batchsize:(i+1)*batchsize] for i in range(int(float(Y_arr.shape[1])/float(batchsize)))]\n",
    "    out_X = out_X + [tail_X]\n",
    "    out_Y = out_Y + [tail_Y]\n",
    "    return(out_X, out_Y)\n",
    "\n",
    "def split_train_val(Z, X, U, order=1, shuff=True, split_prop=0.8):\n",
    "    all_pairs = []\n",
    "    for i in range(Z.shape[1] - order):  #not 8760 for first-diff\n",
    "        all_pairs.append((Z[:,i:i+order-1].flatten(), Z[0:-U.shape[0],i+order]))\n",
    "\n",
    "    if shuff == True:\n",
    "        random.shuffle(all_pairs) \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    train_pairs = all_pairs[0:int(split_prop*len(all_pairs))]\n",
    "    val_pairs = all_pairs[-int((1.0-split_prop)*len(all_pairs)):]\n",
    "    \n",
    "    #normalize to training data\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    for item in train_pairs:\n",
    "        X_train.append(item[0])\n",
    "        Y_train.append(item[1])\n",
    "\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "    for item in val_pairs:\n",
    "        X_val.append(item[0])\n",
    "        Y_val.append(item[1])\n",
    "\n",
    "    X_train = np.asarray(X_train).T\n",
    "    Y_train = np.asarray(Y_train).T\n",
    "\n",
    "    X_val = np.asarray(X_val).T\n",
    "    Y_val = np.asarray(Y_val).T\n",
    "    \n",
    "    return(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "def normalize_data_set(X, Y, X_minmax_vals):\n",
    "    X_norm = copy.copy(X)\n",
    "    Y_norm = copy.copy(Y)\n",
    "    \n",
    "    for row in range(X.shape[0]):\n",
    "        mx = np.max(X_minmax_vals[row,:])\n",
    "        mn = np.min(X_minmax_vals[row,:])\n",
    "\n",
    "        X_norm[row,:] = (1.0/(mx - mn))*(X[row,:] - mn)\n",
    "\n",
    "        if row < Y.shape[0]:\n",
    "            Y_norm[row,:] = (1.0/(mx - mn))*(Y[row,:] - mn)\n",
    "            \n",
    "    return(X_norm, Y_norm)\n",
    "\n",
    "def polynomial_kernel_mat(X, poly_degree):\n",
    "    out = X\n",
    "    for d in range(2,poly_degree+1):\n",
    "        out = np.vstack((out, np.power(X, d)))\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global vars\n",
    "datapath = \"/home/chase/projects/building_transfer/data/eplus_sims/\"\n",
    "\n",
    "#shared simulation var names\n",
    "#dep_var_names = ['Electricity:Facility [J] !Hourly', 'Fans:Electricity [J] !Hourly', 'Cooling:Electricity [J] !Hourly']\n",
    "dep_var_names = ['Fans:Electricity [J] !Hourly']\n",
    "ind_var_names = ['EnvironmentSite Outdoor Air Drybulb Temperature [C] !Hourly', 'EnvironmentSite Outdoor Air Relative Humidity [%] !Hourly']#, 'DayType']\n",
    "\n",
    "zone_temp_inds = [863,864,865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_degrees = [2,3,4,5]\n",
    "orders = [4,5,6,7,8,9,10,11,12]\n",
    "lags = [10,20,30,40,50,60,70,80,90,100,110,120,130,140]\n",
    "\n",
    "prec_values = {}\n",
    "rec_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for poly_degree in poly_degrees:\n",
    "    prec_values[poly_degree] = {}\n",
    "    rec_values[poly_degree] = {}\n",
    "    for order in orders:\n",
    "        prec_values[poly_degree][order] = {}\n",
    "        rec_values[poly_degree][order] = {}\n",
    "        \n",
    "        X_train_pre, Y_train_pre, X_val_pre, Y_val_pre = split_train_val(Z, X, U, order=order, shuff=False, split_prop=1.0)\n",
    "        X_train, Y_train = normalize_data_set(X_train_pre, Y_train_pre, X_train_pre)\n",
    "        X_val, Y_val = normalize_data_set(X_train_pre, Y_train_pre, X_train_pre)\n",
    "\n",
    "        #kernelize\n",
    "        X_train_poly = polynomial_kernel_mat(X_train, poly_degree)\n",
    "        X_val_poly = polynomial_kernel_mat(X_val, poly_degree)\n",
    "\n",
    "        params = {'FEATURE_DIM': X_val_poly.shape[0], 'OUTPUT_DIM': Y_val_pre.shape[0]}\n",
    "        net = linear_nnet(params)\n",
    "        loss_func = nn.MSELoss()#SmoothL1Loss()\n",
    "        optimizer = optim.SGD(net.parameters(),lr=0.01, momentum=0.9)\n",
    "        epochs = 1000\n",
    "        batch_size = 100\n",
    "\n",
    "        for e in range(epochs):\n",
    "            training_losses = []\n",
    "            X_train_list, Y_train_list = minibatch_X_Y_arrays(X_train_poly, Y_train, batch_size)\n",
    "            for i in enumerate(X_train_list):\n",
    "                inp = Variable(torch.Tensor(X_train_list[i[0]].T))\n",
    "                label = Variable(torch.Tensor(Y_train_list[i[0]].T))\n",
    "\n",
    "                out = net(inp)\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_func(out, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        inp_val = Variable(torch.Tensor(X_val_poly.T))\n",
    "        label_val = Variable(torch.Tensor(Y_val.T))\n",
    "        out_val = net(inp_val)\n",
    "        loss_val = loss_func(out_val, label_val)\n",
    "        mape_val = torch.mean(torch.abs(out_val - label_val)/label_val).item()\n",
    "\n",
    "        A = copy.copy(net.l1.weight.data.numpy())\n",
    "        \n",
    "\n",
    "        Z, X, U = read_total_data_array(\"refbuild_med/seattle/RefBldgMediumOfficeNew2004_Chicago.eso\", dep_var_names, ind_var_names, zone_temp_inds, first_diff=True, no_weekend_holiday=True)\n",
    "\n",
    "        X_all_pre, Y_all_pre, X_v, Y_v = split_train_val(Z, X, U, order=order, shuff=False, split_prop=1.0)\n",
    "        X_all, Y_all = normalize_data_set(X_all_pre, Y_all_pre, X_all_pre)\n",
    "\n",
    "        #kernelize\n",
    "        X_all_poly = polynomial_kernel_mat(X_all, poly_degree)\n",
    "        \n",
    "        Z_f, X_f, U_f = read_total_data_array(\"refbuild_med_airfault/seattle/Fault_FoulingAirFilter_RefBldgMediumOfficeNew2004.eso\", dep_var_names, ind_var_names, zone_temp_inds, first_diff=True, no_weekend_holiday=True)\n",
    "\n",
    "        X_all_pre_f, Y_all_pre_f, X_v_f, Y_v_f = split_train_val(Z_f, X_f, U_f, order=order, shuff=False, split_prop=1.0)\n",
    "        X_all_f, Y_all_f = normalize_data_set(X_all_pre_f, Y_all_pre_f, X_all_pre)\n",
    "\n",
    "        #kernelize\n",
    "        X_all_poly_f = polynomial_kernel_mat(X_all_f, poly_degree)\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for i in range(X_all_poly.shape[1]):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"No fault data: \", np.around(100*(i/X_all_poly.shape[1])), \"%\")\n",
    "            x = np.expand_dims(X_all_poly[:,i], axis=1)\n",
    "            y = np.expand_dims(Y_all[:,i], axis=1)\n",
    "            C = mat_C(x)\n",
    "            D = mat_D(x, y, A)\n",
    "            C_inv = np.linalg.inv(C)\n",
    "            term1 = x.shape[0]*np.log(np.linalg.det(C_inv))\n",
    "            term2 = np.trace(C_inv.dot(D.T).dot(D))\n",
    "            term3 = -1.0*np.trace(A.dot(A.T))\n",
    "            term4 = -2.0*np.trace(A.dot(x).dot(y.T))\n",
    "            term5 = np.trace(x.T.dot(A.T).dot(A).dot(x))\n",
    "\n",
    "            feat = [term1, term2, term3, term4, term5]\n",
    "            features_app = np.asarray(feat)#(feat + list(x[:,0]) + list(y[:,0])) #np.expand_dims(A.flatten(), axis=1), x and y themselves seem to be a waste\n",
    "            X.append(features_app)\n",
    "            Y.append(0)\n",
    "\n",
    "        for i in range(X_all_poly_f.shape[1]):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Fault data: \", np.around(100*(i/X_all_poly.shape[1])), \"%\")\n",
    "            x = np.expand_dims(X_all_poly[:,i], axis=1) #tried with fault data\n",
    "            y = np.expand_dims(Y_all_f[:,i], axis=1)\n",
    "            C = mat_C(x)\n",
    "            D = mat_D(x, y, A)\n",
    "            C_inv = np.linalg.inv(C)\n",
    "            term1 = x.shape[0]*np.log(np.linalg.det(C_inv))\n",
    "            term2 = np.trace(C_inv.dot(D.T).dot(D))\n",
    "            term3 = -1.0*np.trace(A.dot(A.T))\n",
    "            term4 = -2.0*np.trace(A.dot(x).dot(y.T))\n",
    "            term5 = np.trace(x.T.dot(A.T).dot(A).dot(x))\n",
    "\n",
    "            feat = [term1, term2, term3, term4, term5]\n",
    "            features_app = np.asarray(feat)#(feat + list(x[:,0]) + list(y[:,0])) #np.expand_dims(A.flatten(), axis=1)\n",
    "            X.append(features_app)\n",
    "            Y.append(1)\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        Y = np.asarray(Y)\n",
    "        Y = np.expand_dims(Y, axis=1)\n",
    "        X = X.T\n",
    "        Y = Y.T\n",
    "        \n",
    "        nofault_samps = []\n",
    "        fault_samps = []\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            if Y[0,i] == 0:\n",
    "                nofault_samps.append(X[:,i])\n",
    "            else:\n",
    "                fault_samps.append(X[:,i])\n",
    "        \n",
    "        for lag in lags:\n",
    "            X_lag_nf = np.zeros((lag*X.shape[0], len(nofault_samps) - lag))\n",
    "            Y_lag_nf = np.zeros((1, len(nofault_samps) - lag))\n",
    "\n",
    "            X_lag_f = np.zeros((lag*X.shape[0], len(fault_samps) - lag))\n",
    "            Y_lag_f = np.ones((1, len(fault_samps) - lag))\n",
    "\n",
    "            for i in range(len(nofault_samps)-lag):\n",
    "                samps = []\n",
    "                for j in range(lag):\n",
    "                    samps.append(nofault_samps[i + j])\n",
    "                X_lag_nf[:,i] = np.asarray(samps).flatten()\n",
    "\n",
    "            for i in range(len(fault_samps)-lag):\n",
    "                samps = []\n",
    "                for j in range(lag):\n",
    "                    samps.append(fault_samps[i + j])\n",
    "                X_lag_f[:,i] = np.asarray(samps).flatten()\n",
    "            \n",
    "            X = np.hstack((X_lag_nf, X_lag_f))\n",
    "            Y = np.concatenate((Y_lag_nf, Y_lag_f), axis=1)\n",
    "            \n",
    "            train_vals = np.random.choice(X.shape[1], int(0.8*X.shape[1]), replace=False)\n",
    "            vals = np.asarray([ i for i in range(X.shape[1]) if i not in train_vals ])\n",
    "\n",
    "            X_train_log_reg = X[:,train_vals] #X[:,train_vals]\n",
    "            Y_train_log_reg = Y[:,train_vals]\n",
    "\n",
    "            X_val_log_reg = X[:,vals]\n",
    "            Y_val_log_reg = Y[:,vals]\n",
    "            \n",
    "            logreg_model_obj = LogisticRegression(tol=0.0001, C=0.9, max_iter=10000, verbose=1)#, n_jobs=-1, solver=\"saga\")\n",
    "            logreg_model_obj.fit(X_train_log_reg.T, Y_train_log_reg.T)\n",
    "\n",
    "            Y_hat = logreg_model_obj.predict(X_train_log_reg.T)\n",
    "\n",
    "            p = sklearn.metrics.precision_score(Y_train_log_reg.T, Y_hat.T)\n",
    "            r = sklearn.metrics.recall_score(Y_train_log_reg.T, Y_hat.T)\n",
    "\n",
    "            print(\"Params: \", poly_degree, order, lag)\n",
    "            \n",
    "            print(\"Training precision: \", p)\n",
    "            print(\"Training recall: \", r)\n",
    "\n",
    "            Y_hat_val = logreg_model_obj.predict(X_val_log_reg.T)\n",
    "\n",
    "            p = sklearn.metrics.precision_score(Y_val_log_reg.T, Y_hat_val.T)\n",
    "            r = sklearn.metrics.recall_score(Y_val_log_reg.T, Y_hat_val.T)\n",
    "\n",
    "            print(\"Validation precision: \", p)\n",
    "            print(\"Validation recall: \", r)\n",
    "            \n",
    "            prec_values[poly_degree][order][lag] = p\n",
    "            rec_values[poly_degree][order][lag] = r\n",
    "            \n",
    "            print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
